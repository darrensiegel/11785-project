# 11785 Project 57 - Darren Siegel

```elixir
Mix.install(
  [
    {:kino_bumblebee, "~> 0.5.0"},
    {:exla, ">= 0.0.0"},
    {:axon, "~> 0.7.0"},
    {:table_rex, "~> 3.1.1"},
    {:polaris, "~> 0.1.0"},
    {:hackney, "~> 1.17"},
    {:table_rex, "~> 3.1.1"}
  ],
  config: [nx: [default_backend: EXLA.Backend]]
)
```

## Project Goal

Assess whether recent advancements in the **Elixir programming language** ecosystem make it a viable alternative to Python for Deep Learning

> Elixir: A functional programming language that runs on the Erlang VM, known for its concurrency, fault tolerance and ease of distributed computing
> 
> Learn more: https://elixir-lang.org/

## Tasks

1. Build and train a model from scratch (by replicating HW1PT2)
2. Fine tune a pre-trained model for a transfer learning task
3. Deploy a trained model for inference into a production Elixir system

The production system: `Torus` (https://github.com/Simon-Initiative/oli-torus), an open source,
course authoring and delivery system developed here at CMU.

<!-- livebook:{"break_markdown":true} -->

### Recent Elixir Advancements (since 2021)

* `Nx` - GPU accelerated tensor library
* `Axon` - Neural network model creation and training
* `Bumblebee` - Pretrained model access
* `Livebook` - Jupyter Notebook for Elixir

## Axon

```elixir
model =
  Axon.input("input")
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dense(256)
  |> Axon.relu()
  |> Axon.dropout(rate: 0.3)
  |> Axon.dense(1)
  |> Axon.sigmoid()

# Axon has support for 1D and 2D convolutions, RNNs, normalizations, activations
# weight initialization, mixed precision, all the common loss functions, various LR schedulers 
# the most common optimizers

template = Nx.template({16, 224}, :f32)
Axon.Display.as_graph(model, template)
```

## Build and Train a Model From Scratch

See Livebook `11785 - HW1PT2`

## Fine Tune a Pre-Trained Model for a Transfer Learning Task

See Livebook `Image Classification`

## Deploy a Model into Production Elixir System

See branch `11785` of the `Torus` codebase at `https://github.com/Simon-Initiative/oli-torus/tree/11785`

## Results

#### Training a new model from scratch

While it was a fun and interesting experience building out HW1PT2 entirely in Elixir, it is clear that the ecosystem isn't at
a place where even moderate scale model training can be easily done.

The Python Deep Learning stack, with its assuredly tens and tens of thousands more engineering hours behind it, is simply more 
robust, more feature rich and more battle tested.

#### Fine-tuning an pre-trained model

While this work proved to be successful and I was indeed able to fine tune ResNet for a transfer learning task, I cannot image doing this again in Elixir. There is just such a massively broader variety of examples and documentation available for PyTorch for this type of task, that it is ridiculous to spend time trying to figure things out in Elixir.

#### Model Deployment

Deploying a model for inference in an Elixir production application is straightforward with `Nx.Serving` providing "out of the box" batch inference.

## Conclusion

While you can indeed do some (limited) Deep Learning work entirely Elixir, why would you?
